% !Mode:: "TeX:UTF-8"

\chapter{绪论}[Introduction]

\section{研究背景与意义}[Background and Significance]

在学习中，记忆发挥着重要的作用。大量基本认知任务依赖于记忆，例如，学生通过阅读材料来学习时，需要从记忆中检索大量背景知识，才能理解材料内容\cite{reisbergCognitionExploringScience2019}。可以说，记忆是学习中重要的一环。Ebbinghaus在1885年通过定量实验分析，发现了遗忘曲线，其刻画了在没有复习的情况下，记忆随着时间衰退的现象\cite{ebbinghausMemoryContributionExperimental1913}。为了克服遗忘，传统的记忆方式是集中重复，即在短时间内大量重复一段材料。有神经科学实验表明，集中重复是一种低效的记忆方式，因为集中重复不能有效地增强神经元连接强度的长期增益效应\cite{kramarSynapticEvidenceEfficacy2012}。与集中重复不同，间隔重复是一种将复习分散到多天内进行的记忆方法。有大量记忆任务实验\cite{cepedaDistributedPracticeVerbal2006}和生物学实验\cite{smolenRightTimeLearn2016}表明，间隔重复能有效地增强长期记忆，在效果上优于集中重复。

近年来，有大量研究表明，在间隔重复中，间隔的长短会影响记忆的效果\cite{cepedaSpacingEffectsLearning2008,delaneySpacingTestingEffects2010}。怎样的间隔安排是最好的，一直是该领域的热点问题。随着教育技术不断发展，间隔重复的应用范围也越来越广，特别是在在线学习平台上为学习者安排复习任务。

\begin{figure}[htb]
    \setlength{\subfigcapskip}{-1bp}
    \centering
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:flashcard:front}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[正面]{\frame{\includegraphics[width=0.45\textwidth]{front}}}}
    \hspace{2em}
    \subfigure{\label{fig:flashcard:back}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[背面]{\frame{\includegraphics[width=0.45\textwidth]{back}}}}
    \end{minipage}
    \vspace{0.2em}
    \caption{墨墨背单词的单词抽认卡}
    \label{fig:flashcard}
\end{figure}

在大多数间隔重复的实践中，材料是以抽认卡的形式呈现的，如图\ref{fig:flashcard}所示，并使用简单的调度表确定每张抽认卡的复习时间。关于如何设计一个更有效的间隔复习调度算法的问题有着丰富的历史。从Leitner盒到第一个间隔重复软件SuperMemo，他们都是以设计者的经验和个人实验为基础，使用简单的规则启发式算法。许多间隔重复软件（Anki, Mnemosyne等）仍然使用这些算法。由于硬编码的参数和缺乏理论证明，这些算法不能适应不同的学习者和材料。同时，它们的性能也不能得到保证。

随着在线学习平台的普及，越来越多的学生在网上学习，这使得收集大规模学习数据成为可能。在大数据的基础上，通过数据挖掘和机器学习构建智能间隔重复系统的研究已经兴起。而复习调度算法是智能间隔重复系统的核心之一。在墨墨背单词这个支持学习者记忆词汇的语言学习应用中，一个高效的间隔重复调度算法可以节省数百万学习者的时间，并帮助他们记住更多的单词。

根据定义，间隔重复天然地产生一系列时间序列事件，然而目前对复习调度算法研究普遍基于人工挑选的特征，且缺乏对时序信息的利用。此外，大部分研究往往只专注于间隔重复系统的某一模块，对其余模块进行了简化处理。这使得调度算法往往运行在过度理想化的环境中，无法用于真实的在线学习平台。

\section{本文的主要工作}[Main work]

本项工作将构建一套完整的间隔重复系统，提高学习者的记忆效率，其包含记忆预测与复习调度两个模块。

复习调度的本质是如何在合适的时机安排复习，以最大化学习者的长期记忆。这要求系统应能预测学习者的长期记忆状态。有多项工作\cite{settlesTrainableSpacedRepetition2016,zaidiAdaptiveForgettingCurves2020}使用回忆概率和记忆半衰期来描述长期记忆，将长期记忆预测建模为一个回归问题。本项工作构建了LSTM-HLR模型，利用学习者在间隔重复过程中产生的时序数据训练，对学习者的记忆半衰期和回忆概率进行预测。

而在复习调度算法的相关研究中，学习者被假定在有限的时间内记忆材料，并期望在截止期限到来时能最大化所记忆的材料数量\cite{reddyAcceleratingHumanLearning2017}。考虑到记忆和遗忘存在随机性，可作为随机最优控制问题来研究。

在本文中，我们根据收集到的数以百万计的记忆数据，建立了用于模拟学习者长期记忆的LSTM-HLR模型。我们设定了一个具有实际意义的明确的优化目标：使学习者形成长期记忆的记忆成本最小化。为了实现这一目标，我们提出了一种新颖的间隔重复调度算法，即SSP-MMC（Stochastic-Shortest-Path-Minimize-Memorization-Cost）。

目前拥有完整时序信息的大规模数据集是清远墨墨教育科技有限公司在墨墨背单词APP上收集的学习者记忆行为数据，考虑到数据限制和项目时间限制，我们没有在其他学习领域测试我们的复习调度算法。

我们的工作为间隔重复调度算法提供了一个更接近实际环境的长期记忆模型，并通过现实世界的数据进行了测试。我们找到了一个可能的间隔重复调度的优化问题和相应的最优化方法。综上所述，本文的主要贡献是：

\begin{itemize}
    \item 建立并公开发布我们的间隔重复日志数据集，这是第一个包含时间序列信息的数据集。
    \item 据我们所知，这是第一个采用时间序列特征来模拟长期记忆的工作。
    \item 在最小化记忆成本方面，LSTM-HLR和SSP-MMC优于最先进的基线。
\end{itemize}

\section{本文的结构安排}[Thesis Outline]

第二章（背景）首先，为了更好地理解这项工作所需的相关理论概念，一一介绍了相关领域的历史发展。然后列出了在这个问题领域已经完成的或与我们试图解决的问题相关的一些工作。

第三章（方法）介绍了系统的组成模块和模块设计，包括记忆预测和复习规划两个部分，和将两者结合起来的间隔重复系统。

第四章（实验）详细介绍了数据集、实验设置，包括参数设置和模型结构。

第五章（结果）给出了实验结果和分析。

第六章（讨论）讨论了结果的含义，并讨论了这项工作可以扩展的方式。它还从可持续性和伦理道德的角度列出了这项工作可能产生的影响。

最后，第七章（结论）对本文的研究结果进行了总结。

\chapter{间隔重复调度算法背景与相关工作}[Background and Related Work]

本工作主要涉及对记忆模型、调度算法的设计，应用了循环神经网络和最优控制相关的工具。因此，本章将先介绍记忆模型和调度算法的研究历史，然后补充循环神经网络和最优控制的基本原理。

\section{记忆模型}[Models of human memory]

\subsection{Ebbinghaus遗忘曲线}[Ebbinghaus forgetting curve]

Ebbinghaus关于遗忘曲线的经典研究表明\cite{ebbinghausMemoryContributionExperimental1913}，如果在首次记忆材料后不进行复习，材料就会逐渐被忘记。遗忘的程度与不进行复习的时间存在函数关系：
\begin{equation}
b = \frac{k}{(\log t)^c + k}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $b$ &再次记忆时节省的时间与第一次记忆所花时间之比，相当于第二次记忆时还保留多少第一次记忆的内容；\\
    &  $t$ &第二次记忆与第一次记忆之间间隔的时间；
\end{tabularx}\vspace{3.15bp}

$c$和$k$这两个参数并未明确定义，但观察其二阶导数可知，$k$越大，遗忘的速度越慢，$c$越大，遗忘的速度越快。Ebbinghaus在它的研究中尚未给出不同的记忆过程对这些参数的影响。

\subsection{Wickelgren广义幂律}[Generalized power law]

Wixted和Carpenter\cite{wixtedWickelgrenPowerLaw2007}指出，回忆的概率随着时间以幂函数衰减：
\begin{equation}
\label{eqn:GPL}
m=\lambda(1+\beta t)^{-\Psi}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\lambda$ &代表初始学习程度的常数；\\
    &  $\beta$ &时间比例因子；\\
    &  $\Psi$ &遗忘率；
\end{tabularx}\vspace{3.15bp}

在这项工作中，Wixted对比了幂函数和指数函数对艾宾浩斯对遗忘曲线的拟合度，发现幂函数能更好地拟合艾宾浩斯的遗忘数据。

在一项使用大数据来预测和改进学习者记忆保留的工作中，研究人员结合广义幂律函数，提出DASH模型来估计学习者对材料的遗忘情况，通过材料的难度、学习者的能力和学习历史来估计式\ref{eqn:GPL}中的参数大小\cite{jonesPredictingImprovingMemory2016}。

\subsection{ACT-R陈述性记忆模块}[ACT-R]

Pavlikp和Anderson\cite{pavlikUsingModelCompute2008}使用以下模型来描述遗忘过程
\begin{equation}
m_\mathrm{n}(t_\mathrm{1..n}) = \beta + \ln(\sum\limits_{k=1}^n t_\mathrm{k}^{-d_\mathrm{k}})
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $m$ &记忆的强度；\\
    & $d$ &第$k$次复习时的记忆强度衰减率；\\
    & $\Psi$ &遗忘率；
\end{tabularx}\vspace{3.15bp}
\begin{equation}
p(m) = \frac{1}{1+e^{\frac{\tau-m}{s}}}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\tau$ &激活阈值参数；\\
    & $s$ &控制激活中噪声的影响，描述了回忆对激活变化的敏感性；
\end{tabularx}\vspace{3.15bp}

适应性思维特征-理性模型（Adaptive Control of Thought—Rational, ACT-R）\cite{andersonIntegratedTheoryMind2004}是一种认知体系结构，其包含了陈述性记忆模块，假设每次练习会尝试一条幂函数遗忘曲线，多个幂函数近似了间隔重复后的遗忘曲线。

\subsection{多尺度上下文模型}[MCM]

Pashler等人\cite{pashlerPredictingOptimalSpacing2009}通过结合两种心理学模型，得到了以下公式：
\begin{equation}
    s_{N}(t)=\sum_{i=1}^{N} \gamma_\mathrm{i} \exp \left(-\frac{t}{\tau_\mathrm{i}}\right) x_\mathrm{i}(0)
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\tau$ &激活阈值参数；\\
    & $s$ &控制激活中噪声的影响，描述了回忆对激活变化的敏感性；
\end{tabularx}\vspace{3.15bp}

多尺度语境模型（MCM）\cite{pashlerPredictingOptimalSpacing2009}假设每次练习都会产生一条指数函数遗忘曲线，使用多个指数函数叠加来近似遗忘曲线。

\subsection{半衰期回归模型}[HLR]\label{sec:HLR}

Settle和Meeder\cite{settlesTrainableSpacedRepetition2016}提出了记忆半衰期的概念，并用指数函数来刻画回忆概率与时间的关系：
\begin{equation}
p = 2^{-\Delta/h}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $p$ &正确回忆一个项目（例如，一个单词）的概率；\\
    & $\Delta$ &上次练习该项目以来的滞后时间；\\
    & $h$ &学习者长期记忆中的半衰期或强度度量；
\end{tabularx}\vspace{3.15bp}

当$\Delta=h$时，滞后时间等于半衰期，$p=2^{-1}=0.5$时，学生处于遗忘的边缘。这项工作假设回忆只能是二进制的，即回忆成功或回忆失败。

假设半衰期应随每次重复曝光呈指数增加。估计的半衰期$\hat{h}_{\bm\Theta}$由下式给出：
\begin{equation}
\begin{aligned}
\hat{h}_{\bm\Theta}&=2^{\bm\Theta \cdot \bm x}\\
\bm x &= (x_\mathrm{\oplus},x_\mathrm{\ominus}, \bm lex)
\end{aligned}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\bm x$ &描述学生-项目对的学习历史的特征向量；\\
    & $\bm \theta$ &对应于$\bm x$中的每个特征变量的权重；\\
    & $x_\mathrm{\oplus}$ &学习对该项目历史回忆正确的次数；\\
    & $x_\mathrm{\ominus}$ &学习对该项目历史回忆错误的次数；\\
    & $\bm lex$ &词位向量，代表单词的标签；
\end{tabularx}\vspace{3.15bp}

由于HLR挑选的特征向量只包含统计信息，无法捕捉记忆在时序上的特征。

HLR模型通过机器学习的方式来训练$\bm \theta$的大小。损失函数为：
\begin{equation}
\label{eqn:hlr:loss}
loss=(p-\hat p)^2 + \alpha(h-\hat h)^2 + \lambda||\bm\theta||^2
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\hat p$ &回忆概率预测值；\\
    & $\hat h$ &记忆半衰期预测值；\\
    & $\lambda$ &正则化系数；
\end{tabularx}\vspace{3.15bp}

虽然HLR模型考虑了降低记忆半衰期的误差，但仅考虑了绝对误差，而没有考虑绝对百分比误差更接近实际情况。

\section{调度算法}[Algorithms of spaced repetition]

\subsection{Pimsleur方法}[Pimsleur Method]\label{sec:pimsleur}

Pimsleur提出在外语学习中使用递增间隔进行记忆，并刻画间隔重复下遗忘曲线的变化\cite{pimsleurMemorySchedule1967}。Pimsleur的复习间隔以5为底指数增长，即5秒、25秒、125秒……这套复习间隔与其基于音频的语言学习程序相结合，形成了Pimsleur方法。然而，由于这套复习周期是预先录制的，其无法适应学习者的实际能力和材料的实际难度。例如，一个中国学生可能会非常容易地记住apple，但很难记住archaeology。但在Pimsleur方法中，他必须按照固定的、以相同速度增长的复习间隔来复习这两个单词。

\subsection{Leitner系统}[Leitner Box]

Leitner提出了一种基于纸质抽认卡和盒子的间隔重复调度算法——Leitner盒子\cite{leitnerLerntManLeben1974}。通过将卡片在代表不同熟练度的盒子之间进行转移，加上为不同盒子安排不同的复习周期，Leitner盒子表现出更强的适应能力。目前依然有不少基于Leitner盒子的间隔重复软件，并对其进行了改进。最常见的Leitner盒子变体是，准备若干个盒子，对应不同的复习间隔，如“1天”、“2天”、“4天”。所有的抽认卡会先放入“1天”的盒子内容。然后每天学习者需要复习“1天”盒子内容的卡片，并对每张卡片进行反馈。记住的卡片将会被放入“2天”的盒子。学习者每两天复习一次“2天”盒子，每四天复习一次“4天”盒子。在复习过程中被忘记的卡片将会被放回“1天”盒子，如图\ref{fig:leitner}所示。用同样的例子来说明，\ref{sec:pimsleur}节中的中国学生可能会因为记不住archaeology而将记有该单词的卡片放回“1天”盒子，相比apple进行更频繁的复习。

\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.6\textwidth]{Leitner}
    \caption{Leitner盒子}
    \label{fig:leitner}
\end{figure}

\subsection{SuperMemo 2算法}[SuperMemo]

Wozniak提出了首个运行于计算机的间隔重复算法SM-2\cite{wozniakOptimizationLearning1990}，其通过学习者对记忆的评分来评估电子抽认卡的难易程度，然后基于难易程度和学习者的评分来计算复习间隔。

\begin{algorithm}[htbp]
    \KwData{$g, n ,I, EF$}
    \KwResult{$n, I, EF$}
    \eIf{$g>=3$}{
        \uIf{$n==0$}{
            $n \leftarrow 1$\;
            $I \leftarrow 1$\;
        }
        \uElseIf{$n==1$}{
            $n \leftarrow 2$\;
            $I \leftarrow 6$\;
        }
        \Else{
            $n \leftarrow n+1$\;
            $I \leftarrow round(I\cdot EF)$\;
        }
    }{
        $n \leftarrow 0$\;
        $I \leftarrow 1$\;
    }
    $EF \leftarrow  EF + (0.1-(5-g)\cdot(0.08+(5-g)\cdot0.02))$\;
    \uIf{$EF<1.3$}{$EF \leftarrow 1.3$\;}
\caption{SuperMemo 2}
\label{alg:sm-2}
\end{algorithm}

SM-2的伪代码如算法\ref{alg:sm-2}所示，其中表示学习者对此次复习的评分，范围为0~5，大于等于3视为回忆成功。$n$表示学习者连续回忆成功的次数。$I$表示学习者复习的间隔。$EF$表示该记忆内容的简易程度。

根据SM-2模型，可以得出一个重复的间隔序列：1天、6天、15天、38天……以此类推。如果学习者在复习过程中遗忘，间隔将重新从1天开始。并且由于简易度的下降，新的间隔序列整体短于上一次的序列，从而让学习者对每张抽认卡的回忆概率逐步提高，直到学习者能较大概率回忆起这些抽认卡。

SM-2模型将学习者的复习间隔和回忆评分作为输入，引入简易度作为中间变量，通过硬编码的模型计算下一次间隔，实现了对不同抽认卡的独立跟踪和间隔安排。该模型的优点在于部分考虑了记忆行为的序列特征。其局限性在于，由于模型是根据经验硬编码的，不能定量地预测学习者的遗忘情况。并且模型对简易度的迭代调整较小，导致间隔的收敛速度也较为缓慢。

\subsection{Memorize算法}[Memorize]

Tabibian等人\cite{tabibianEnhancingHumanLearning2019}引入了标记的时间点过程来表示间隔重复，并将调度视为一个最佳控制问题。

\begin{equation}
\begin{aligned}
    m_\mathrm{i}(t)&=\mathbb{P}(r)=\exp (-n_\mathrm{i}(t)(t-t_\mathrm{r}))\\
    \mathrm{d} n_\mathrm{i}(t)&=-\alpha_\mathrm{i} n_\mathrm{i}(t) r_\mathrm{i}(t) \mathrm{d} N_\mathrm{i}(t)+\beta_\mathrm{i} n_\mathrm{i}(t)(1-r_\mathrm{i}(t)) \mathrm{d} N_\mathrm{i}(t)\\
    \mathrm{d} m_\mathrm{i}(t)&=-n_\mathrm{i}(t) m_\mathrm{i}(t) \mathrm{d} t+(1-m_\mathrm{i}(t)) \mathrm{d} N_\mathrm{i}(t)
\end{aligned}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $m_\mathrm{i}(t)$ &$t$时刻项目$i$的回忆概率；\\
    & $t-t_\mathrm{r}$ &上次复习的时间点；\\
    & $n_\mathrm{i}(t)$ &$t$时刻项目$i$的遗忘率；\\
    & $N_\mathrm{i}(t)$ &$t$时刻项目$i$的复习次数；\\
    & $\alpha_\mathrm{i}$ &回忆成功时遗忘率的变化率；\\
    & $\beta_\mathrm{i}$ &回忆失败时遗忘率的变化率；\\
\end{tabularx}\vspace{3.15bp}

通过使用带跳变的随机微分方程刻画记忆的动态，Memorize算法提出了以下优化目标，并应用随机最优控制的方法解决：

\begin{equation}
    \ell(m(t), n(t), u(t))=\frac{1}{2}(1-m(t))^{2}+\frac{1}{2} q u^{2}(t)
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $q$ &权衡回忆概率和复习次数的参数；\\
    & $u$ &复习强度；
\end{tabularx}\vspace{3.15bp}

结合最小化遗忘率和复习强度与记忆系统动态，Memorize调度算法在一定程度上拥有了处理间隔重复过程中时序信息的能力。但其系统动态没有考虑每次复习之间的间隔对遗忘率的影响。

\subsection{基于强化学习的方法}[RL]

一些论文\cite{reddyAcceleratingHumanLearning2017,upadhyayDeepReinforcementLearning2018,sinhaUsingDeepReinforcement2019,yangTADSLearningTimeaware2020}通过设计奖励和在模拟环境中的训练，使用RL来最大化学习者的记忆期望。

通用的马尔科夫决策过程的优化问题可以表述如下：
\begin{equation}
    \pi^{*}=\underset{\pi \in \Pi}{\operatorname{argmax}} \mathbb{E}_{s_{0}, a_{0}, \ldots}\left[\sum_{t=0}^{\infty} \gamma^{t} R(s_{t}, a_{t}) \mid \pi\right]
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\pi$ &策略；\\
    & $R$ &奖励函数；\\
    & $s$ &状态；\\
    & $a$ &行动；\\
    & $\gamma$ &衰减系数；
\end{tabularx}\vspace{3.15bp}

在间隔重复中，状态即学习者此刻的所有记忆状态，行为则是此刻选择哪些记忆进行复习巩固。而奖励函数通常被定义为当前所有记忆的回忆期望。最优的间隔重复策略旨在最大化所有记忆的回忆期望。

\section{循环神经网络}[Recurrent Neural Network]

\subsection{简单循环网络}[SRN]

现实中的很多任务输入在时间上不是独立的，比如语音、语言，并且这些任务的时序数据长度通常是不固定的。传统的前馈网络难以解决这些问题，于是循环神经网络运用而生。循环神经网络中的神经元同时接受其他神经元和自己过去的信息，形成了带环的网络结构，这赋予了循环神经一定的短期记忆能力。

在间隔重复中，同一位学习者对同一条材料在不同时刻的复习之间是不独立的，循环神经网络正适合用于这种场景。通过输入记忆行为时序数据，循环神经网络可用于预测记忆的状态变化。

如图\ref{fig:SRN}所示，一个简单的RNN是在一个两层的前馈神经网络中加入一个隐藏层实现的。除了相邻的层之间有连接，隐藏层自身到自身也存在反馈连接。这使得当前时刻的隐藏层状态不仅与当前时刻的输入有关，也与上一时刻的隐藏层状态相关。

\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.45\textwidth]{SRN}
    \caption{简单循环神经网络\cite[137]{qiu2020nndl}}
    \label{fig:SRN}
\end{figure}

尽管RNN可以学习使用过去的相关信息，但在实践中，它们很难捕获长程依赖关系。为了改善该问题，引入门控机制来控制信息在循环中的积累与遗忘，是一个很好的解决方案。长短期记忆网络\cite{hochreiterLongShortTermMemory1997}和门控循环单元网络\cite{choPropertiesNeuralMachine2014}是这类方案的常见实现。

\subsection{长短时记忆网络}[LSTM]

LSTM的整体链状结构非常类似于RNN，但LSTM中单元的内部结构比RNN中单元的内部结构稍微复杂一些。LSTM通过引入一个新的内部状态$c$专门进行线性的循环信息传递，同时非线性地输出信息给隐藏层。同时，LSTM使用门控机制来控制信息传递的路径，包括遗忘门、输入门、输出门，每个门都有各自的用途。遗忘门确定前一存储单元的哪个部分应该被带到下一步，而输入门决定哪些值将被更新。输出门确定单元状态的哪些部分应该提供输出。

在RNN中，隐藏层的状态$h$存储了历史信息，并在每个时刻被更新，因此可以看作一种短期记忆。而神经网络的网络参数隐含了从训练数据中学到的经验，可看作一种长期记忆。在LSTM中，记忆单元$c$能够将关键信息保存一定时间，其生命周期长于短期记忆$h$，又短于长期记忆，因此LSTM能够捕捉长短期记忆。

\subsection{门控循环单元网络}[GRU]

GRU是RNN的另一种类型，也具有门控体系结构。它具有通过将遗忘门和输入门组合而形成的更新门。相比LSTM，GRU还有一些额外的变化，如单元状态$c$和隐藏状态$h$的整合。GRU的结构相比LSTM更为简单，同时也改善了RNN的长程依赖问题。

\section{随机最优控制}[Stochastic Optimal Control]

\subsection{确定性问题的动态规划}[Deterministic Dynamic Programming]

对于一个离散时间的动态系统，每个时刻的系统状态与上一时刻的状态和决策有关。可用一个函数来描述系统动态特性。已知系统初始状态、系统动态特性和决策序列，就可以计算出任意时刻的系统状态变量\cite[2-4]{bertsekasReinforcementLearningOptimal2019}。

在动态系统的发展过程中，系统的使用者往往有所期待。例如，对于间隔重复系统，学习者希望自己记忆材料付出的时间最少。通过引入代价函数来描述成本与系统状态、决策之间的关系，然后对每个时刻的代价进行累加，可得到总的代价值。描述总代价的函数即确定性问题的目标函数。

对于动态规划可求解的问题，需要代价函数只依赖于当前时刻的状态和决策，以满足最优性原理。对于确定性系统，动态规划将求解每一个状态下的最优决策量，以最小化目标函数。

\subsection{随机性问题的动态规划}[Stochastic Dynamic Programming]

然而，在间隔重复中，学习者的记忆反馈存在随机性，因此间隔重复系统是一个有随机性的动态系统。

针对具有随机性的动态系统，动态规划将求解一个最优策略而非给出决策量的值。同时，在计算每个状态的最优策略时，随机性问题的目标函数是带期望的\cite[14-16]{bertsekasReinforcementLearningOptimal2019}。

\subsection{无限阶段问题的动态规划}[Infinite Horizon Dynamic Programming]

考虑到记忆的随机性，无法保证记忆的状态能够在有限阶段内达到学习者期望的目标。因此，该对动态系统进行控制最优化需要考虑无穷多阶段的问题。这要求在动态规划求解过程中，需要遍历所有状态值。

通过使用分布式异步值迭代方法，每次只需要更新一个状态对应的值函数，也能让值函数收敛到最优\cite[197-200]{bertsekasReinforcementLearningOptimal2019}。

\section{本章小结}[Summary]

在优化间隔重复方面已经有大量的文献，从建模和预测学习者的长期记忆到基于相关记忆模型设计优化调度算法。

间隔重复和记忆模型。适应性思维特征-理性模型（Adaptive Control of Thought—Rational, ACT-R）\cite{andersonIntegratedTheoryMind2004}是一种认知体系结构，其包含了陈述性记忆模块，假设每次练习会尝试一条幂函数遗忘曲线，多个幂函数近似了间隔重复后的遗忘曲线。而多尺度语境模型（MCM）\cite{pashlerPredictingOptimalSpacing2009}，假设每次练习都会产生一条指数函数遗忘曲线，使用多个指数函数叠加来近似遗忘曲线。半衰期回归模型（HLR）\cite{settlesTrainableSpacedRepetition2016}是一个可训练的间隔重复模型。为了预测学习者对特定材料的记忆半衰期，在线语言学习平台Duolingo的研究者使用他们的学习者日志数据进行训练。Ahmed Zaidi等人\cite{zaidiAdaptiveForgettingCurves2020}在该模型上进行了改进，将神经网络引入了该模型，并考虑了更多心理学、语言学相关的特征。但这些模型没有考虑学习者对单词记忆的反馈顺序和反馈之间的间隔，缺失了时序信息。

用间隔重复模型进行优化调度。为了平衡新材料的学习和已学材料的复习，Reddy等人\cite{reddyUnboundedHumanLearning2016}提出了Leitner系统\cite{leitnerLerntManLeben1974}的排队网络模型，并设计了一种启发式算法来安排复习。然而，该算法是基于EFC模型\cite{reddyUnboundedHumanLearning2016}，将记忆强度作为复习次数和Leitner箱位置的函数，而不是重复之间的间隔。Tabibian等人\cite{tabibianEnhancingHumanLearning2019}引入了标记的时间点过程来表示间隔的重复，并将调度视为一个最佳控制问题。他们想出了回忆概率和复习数量之间的权衡。由于数据集的限制，他们模型中的遗忘率只受回忆结果的影响。在Hunziker等人\cite{hunzikerTeachingMultipleConcepts2019}的工作中，优化间隔重复的调度归结为随机序列优化问题。他们设计了一种贪婪的算法来实现学习者保持率最大化的高性能。但该算法实现高效用的充分条件很严格，可能在大多数情况下不适用。最近，强化学习也被应用于优化间隔重复的安排。一些论文\cite{reddyAcceleratingHumanLearning2017,upadhyayDeepReinforcementLearning2018,sinhaUsingDeepReinforcement2019,yangTADSLearningTimeaware2020}通过设计奖励和在模拟环境中的训练，使用RL来最大化学习者的记忆期望。然而，他们的算法所基于的环境过于简化，使得它不能很好地适用于现实世界。除此之外，这些算法缺乏可解释性，在不同的模拟环境中，它们的表现也不尽相同，并不总是优于启发式算法。

循环神经网络和最优控制是解决间隔重复调度问题的合适工具。首先，循环神经网络在处理时序数据上有很大的优势，特别是在间隔重复中，人类的记忆变化不仅仅依赖于单次复习的结果，也与复习时的记忆状态有关。而循环神经网络通过将历史输入编码成一个隐含状态，新一时刻的输入与隐含状态共同决定了当前输出和新的隐含状态，与人类的记忆十分类似。其次，最优控制非常适用于有明确系统动态和优化目标的问题。在间隔重复中，记忆的动态变化可以由循环神经网络捕获，优化的目标则与学习者的实际需求相关。通过将间隔重复问题化为一个无限阶段的随机动态规划问题，最优控制能够给出可解释的解决方法。

\chapter{间隔重复调度算法构建方法}[Method]

\section{记忆预测：LSTM-HLR模型}[Predict memory]

\subsection{收集数据}[Dataset]

我们收集了墨墨背单词一个月的日志，包含2.2亿条记忆行为数据，以建立一个模型来模拟学习者的记忆。由于以下原因，我们没有使用Duolingo的开源数据集：

\begin{itemize}
    \item 它缺乏时间序列方面的内容，如反馈和间隔的序列，而我们对数据的分析表明，历史特征对记忆状态有很大的影响。
    \item 它的回忆概率定义存在问题。Duolingo将回忆概率定义为一个单词在特定会话中被正确回忆的次数比例，这意味着同一单词在特定会话中的多次记忆行为是独立的。然而，第一次回忆会影响学习者的记忆状态和全天的后续记忆。
\end{itemize}

从墨墨背单词收集的记忆数据记录了学习者对单词记忆的历史信息。对于任意一个记忆行为，我们用一个四元组来表示：
\begin{equation}
e :=(u, w, t, r)
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $e$ &记忆行为事件；\\
    &  $u$ &学习者；\\
    &  $t$ &学习发生的时刻；\\
    &  $w$ &学习的单词；\\
    &  $r$ &学习的反馈；
\end{tabularx}\vspace{3.15bp}
其含义为一名学习者$u$在时刻$t$回忆单词$w$并反馈$r$（回忆成功$r=1$；回忆失败$r=0$）的事件。

为了便于研究记忆行为事件序列，我们将历史序列特征加入其中：
\begin{equation}
\label{eqn:event}
e_\mathrm{i} :=(u, w, \bm{\Delta t_\mathrm{1:i-1}}, \bm r_\mathrm{1:i-1} , \Delta t_\mathrm{i} , r_\mathrm{i})
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $e_\mathrm{i}$ &学习者$u$对单词$w$的第$i$次记忆行为事件；\\
    &  $\Delta t$ &学习者对同一个单词的两次相邻回忆事件之间的时间间隔；\\
    &  $\bm \Delta t_\mathrm{1:i-1}$ &第$1$次到第$i-1$次回忆的间隔历史序列；\\
    &  $\bm r_\mathrm{1:i-1}$ &第$1$次到第$i-1$次回忆的反馈历史序列；
\end{tabularx}\vspace{3.15bp}

我们过滤掉其中第一个反馈为$r=1$的日志，以排除学习者在使用间隔重复之前形成的记忆的影响。学习者的记忆行为事件的样本日志见表\ref{tab:raw}。

\begin{table}[htbp]
    \caption{数据集样本示例}
    \label{tab:raw}
    \vspace{0.5em}\centering\wuhao
    \begin{tabular}{llllll}
    \toprule[1.5pt]
    $u$    & $w$      & $\bm{\Delta t_\mathrm{1:i-1}}$ & $\bm r_\mathrm{1:i-1}$ & $\Delta t_\mathrm{i}$ & $r_\mathrm{i}$ \\ 
    \midrule[1pt]
    23af1d & solemn   & 0,1,3,1,3,6,10                  & 0,1,0,1,1,1,0           & 1            & 0 \\
    23af1d & dominate & 0                               & 0                       & 1            & 1 \\
    e9654e & nursery  & 0,1,1,3,1,3                     & 0,0,1,0,1,1             & 1            & 1 \\ 
    \bottomrule[1.5pt]
    \end{tabular}
\end{table}

由此我们得到了包含任意学习者对任意单词的完整记忆行为历史数据。接下来，我们将基于该数据和遗忘曲线，从而引出我们对记忆半衰期的定义。

\subsection{记忆半衰期}[Memory Half-life]

遗忘曲线是说，学习者停止复习后，记忆会随着时间的推移而衰退。在公式\ref{eqn:event}定义的记忆行为事件中，回忆是二元的（即，学习者要么完全回忆起一个单词，要么忘记一个单词）。为了捕捉记忆的衰退，我们需要得到二元回忆背后潜在的概率。为了得到回忆概率，我们忽略学习者本身的影响，用学习该单词的学习者中的回忆比例$\frac{n_\mathrm{r=1}}{N}$作为回忆概率$p$，从而聚合得到:
\begin{equation}
e_\mathrm{i} :=(w, \bm{\Delta t}_\mathrm{1:i-1}, \bm r_\mathrm{1:i-1}, \Delta t_\mathrm{i} , p_\mathrm{i}, N)
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $p_\mathrm{i}$ &有相同记忆行为历史下的学习者群体中的回忆成功比例；\\
    &  $N$ &拥有相同记忆行为历史下的学习者数量；
\end{tabularx}\vspace{3.15bp}

通过控制$w$、$\bm{\Delta t}_\mathrm{1:i-1}$和$\bm r_\mathrm{1:i-1}$，我们可以绘制每个复习间隔$\Delta t$对应的回忆概率$p$，从而得到遗忘曲线。当$N$足够大时，比例$\frac{n_\mathrm{r=1}}{N}$接近回忆概率。然而，墨墨背单词中有将近10万个单词，在不同的时间序列中为每个单词收集的行为事件是稀疏的。我们需要对单词进行分组，以便在区分不同的单词和缓解数据稀疏性之间做出权衡。鉴于我们对遗忘曲线感兴趣，而材料的难度会明显影响遗忘速度。因此，我们尝试用第一次学习它们后第二天的回忆比例作为划分单词难度的标准。而图\ref{fig:distribution:p}描绘了数据集中回忆比例的分布。

\begin{figure}[htbp]
    \setlength{\subfigcapskip}{-1bp}
    \centering
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:distribution:p}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[回忆概率]{\includegraphics[width=0.45\textwidth]{distribution_p}}}
    \hspace{2em}
    \subfigure{\label{fig:distribution:d}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[难度]{\includegraphics[width=0.45\textwidth]{distribution_d}}}
    \end{minipage}
    \caption{回忆概率和难度的分布}
    \label{fig:distribution}
\end{figure}

我们可以从数据分布中看到，回忆比例大多在0.45和0.85之间。为了使分组数据的平衡和密度，将单词分为10个难度组。分组的结果显示在图\ref{fig:distribution:d}和表\ref{tab:difficulty}中。符号$d$表示难度，数字越大，难度就越大。

\begin{table}[htbp]
    \caption{单词难度分组样例}
    \label{tab:difficulty}
    \vspace{0.5em}\centering\wuhao
    \begin{tabular}{lllllll}
    \toprule[1.5pt]
    $w$          & $d$  & $\bm{\Delta t}_\mathrm{1:i-1}$ & $\bm r_\mathrm{1:i-1}$& $\Delta t_\mathrm{i}$ & $p_\mathrm{i}$& $N$ \\ 
    \midrule[1pt]
    automobile & 2  & 0          & 0          & 1        & 0.8379    & 7501       \\
    temper     & 7  & 0          & 0          & 1        & 0.5867    & 7498       \\
    emission   & 8  & 0          & 0          & 1        & 0.5094    & 7497       \\
    multiply   & 6  & 0          & 0          & 1        & 0.635     & 7495       \\
    linger     & 9  & 0          & 0          & 1        & 0.4925    & 7493       \\
    hatch      & 10 & 0          & 0          & 1        & 0.4371    & 7492       \\ 
    \bottomrule[1.5pt]
    \end{tabular}
\end{table}

使用分组后的数据，我们便可以绘制有足够数据支持的遗忘曲线。我们基于指数遗忘曲线模型$p_\mathrm{i} = 2^{-\frac{\Delta t_\mathrm{i}}{h_\mathrm{i}}}$对其进行拟合，并由此得到记忆半衰期$h_\mathrm{i}$。

通过此方法，我们将$\Delta t_\mathrm{i} , p_\mathrm{i}$特征降维得到$h_\mathrm{i}$，再将聚合数据得到的回忆概率$p_\mathrm{i}$按照时间顺序进行拼接，得到回忆概率序列 $\bm p_\mathrm{1:i-1}$作为特征之一。最终，数据实例为：
\begin{equation}
\label{eqn:event:final}
e_\mathrm{i}:=(d, \bm{\Delta t}_\mathrm{1:i-1}, \bm r_\mathrm{1:i-1}, \bm p_\mathrm{1:i-1}, \Delta t_\mathrm{i} , p_\mathrm{i}, h_\mathrm{i})
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $d$ &单词难度分组；\\
    &  $\bm p_\mathrm{1:i-1}$ &第$1$次到第$i-1$次回忆的回忆比例历史序列；\\
    &  $h_\mathrm{i}$ &学习者群体的记忆半衰期；
\end{tabularx}\vspace{3.15bp}

\subsection{模型表示}[Model]\label{sec:LSTM}

我们采用了最简单LSTM网络，一层输入层、一层隐藏层、一层全连接层。网络的输入输出可描述如下：
\begin{equation}
\hat{h_\mathrm{i}}=\mathrm{LSTM}(\bm{\Delta t}_\mathrm{1:i-1}, \bm r_\mathrm{1:i-1}, \bm p_\mathrm{1:i-1})
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中&  $\hat{h_\mathrm{i}}$ &第$i$次回忆时的记忆半衰期预测值；\\
    &  $\mathrm{LSTM}$ &LSTM模型；
\end{tabularx}\vspace{3.15bp}

从由式\ref{eqn:event:final}定义的数据中取出3个通道时序特征用于训练LSTM网络，以拟合不同记忆行为历史下的记忆半衰期。模型的目标是降低记忆半衰期的预测误差，因此设置损失函数如下：
\begin{equation}
l(e_\mathrm{i},\bm\theta)=|\frac{h_\mathrm{i}-\hat{h_\mathrm{i}}}{h_\mathrm{i}}|+C||\bm\theta||_{2}^{2}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $l$ &损失函数；\\
    &  $C$ &正则化系数；\\
    &  $\bm\theta$ &模型权重；
\end{tabularx}\vspace{3.15bp}

之所以我们采用$h$的相对百分比误差作为损失项，是因为相同的绝对误差在不同的记忆半衰期上的代价是不同的。举例来说，10天的绝对误差，对于一个真实半衰期为1天的记忆行为，将会导致回忆概率误差$|p-\hat{p}|=|2^{-\frac{1}{1}}-2^{-\frac{1}{11}}|\approx 0.4389$，而对一个真实半衰期为100天的记忆行为，其导致的回忆概率误差$p-\hat{p}=2^{-\frac{100}{100}}-2^{-\frac{100}{110}}\approx 0.0325$。

结合\ref{sec:HLR}节的HLR模型，对于任何一条数据实例，都可由下式预测学习者的回忆概率：
\begin{equation}
    \hat{p_\mathrm{i}}=2^{-\frac{\Delta t_\mathrm{i}}{\hat{h_\mathrm{i}}}}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\hat{p_\mathrm{i}}$ &第$i$次回忆时的回忆概率预测值；
\end{tabularx}\vspace{3.15bp}

\section{复习规划：SSP-MMC算法}[Algorithm]

\subsection{问题设置}[Problem Setting]

间隔重复方法的目的在于帮助学习者高效地形成长期记忆。而记忆半衰期反映了长期记忆的存储强度，复习次数、每次复习所花费的时间则反映了记忆的成本。所以，间隔重复调度优化的目标可以表述为：在给定记忆成本约束内，尽可能让更多的材料达到目标半衰期，或以最小的记忆成本让一定量的记忆材料达到目标半衰期。其中，后者的问题可以简化为如何以最小的记忆成本让一条记忆材料达到目标半衰期，即最小化记忆成本（Minimize Memorization Cost, MMC）。

我们在\ref{sec:LSTM}节所构建的长期记忆模型满足马尔可夫性，每次记忆的状态只取决于上一次的记忆状态和当前的复习输入和结果。其中回忆结果服从一个与复习间隔有关的随机分布。由于记忆状态转移存在随机性，一条记忆材料达到目标半衰期所需的复习次数是不确定的。因此，间隔重复调度问题可以视作一个无限阶段的随机动态规划问题。考虑优化目标是让记忆半衰期达到目标水平，所以本问题存在终止状态，可以转化为一个随机最短路径问题\cite[177-182]{bertsekasReinforcementLearningOptimal2019}（Stochastic Shortest Path, SSP）。结合优化目标，我们将算法命名为SSP-MMC。

\subsection{状态表示}[State]

为了解决转化为随机最短路径问题的间隔重复最小化记忆成本问题，首先要解决的是如何描述学习者长期记忆的系统动态。

根据\ref{sec:LSTM}节的LSTM-HLR模型，训练好的LSTM网络在预测记忆行为序列的记忆半衰期过程中，使用记忆单元$\bm c_\mathrm{k}$和隐藏层$\bm h_\mathrm{k}$来记录当前时刻的状态。根据网络结构，记忆单元和隐藏层的值只依赖于当前时刻的输入值和上一时刻自身的值。如果将记忆单元和隐藏层视作记忆状态，那么记忆状态$\bm x_\mathrm{k}$和复习行为$u_\mathrm{k}$之间的关系可用马尔可夫决策过程描述。

记忆状态可描述如下：
\begin{equation}
\bm x_\mathrm{k} = (\bm c_\mathrm{k},\bm h_\mathrm{k})
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\bm x_\mathrm{k}$ &第$k$次复习时的记忆状态；\\
    &  $\bm c_\mathrm{k}$ &预测第$k$次复习时LSTM网络的记忆单元状态；\\
    &  $\bm h_\mathrm{k}$ &预测第$k$次复习时LSTM网络的隐藏层状态；
\end{tabularx}\vspace{3.15bp}

复习行为可描述如下：
\begin{equation}
u_\mathrm{k}=\Delta t_\mathrm{k}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $u_\mathrm{k}$ &第$k$次复习后规划的复习行为；\\
    &  $\Delta t_\mathrm{k}$ &第$k$次复习后规划的复习间隔；
\end{tabularx}\vspace{3.15bp}

随机变量可描述如下：
\begin{equation}
r_\mathrm{k} \sim B(p_\mathrm{k})
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $r_\mathrm{k}$ &第$k$次复习的回忆反馈；\\
    &  $B$ &伯努利分布；\\
    &  $p_\mathrm{k}$ &第$k$次复习的回忆概率；
\end{tabularx}\vspace{3.15bp}

系统动态可描述如下：
\begin{equation}
\bm x_\mathrm{k+1}=\mathrm{LSTM}(\bm x_\mathrm{k},u_\mathrm{k},r_\mathrm{k})
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\bm x_{k+1}$ &在记忆状态$\bm x_\mathrm{k}$、复习行为$u_\mathrm{k}$和记忆反馈$r_\mathrm{k}$下的下一个记忆状态；
\end{tabularx}\vspace{3.15bp}

通过引入决策成本和终止状态$\bm x_N$，可得到图\ref{fig:ssp}描述的随机最短路径问题。图中的圆圈代表了记忆状态，方块是可选的复习行为，虚线箭头表示给定复习行为后的下一个记忆状态和转换概率，实线箭头表示给定记忆状态下可选的复查行为与相应的复习成本。通过计算不同记忆状态对应的记忆半衰期，可得到达到目标半衰期的记忆状态集合。该集合内的记忆状态节点即终止状态。而间隔重复中的随机最短路径问题是调度算法如何建议复习行为，以最小化达到终止记忆状态的期望复习成本。

\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.9\textwidth]{ssp}
    \caption{间隔重复中的随机最短路径问题}
    \label{fig:ssp}
\end{figure}

\subsection{算法设计}[Design]

根据马尔可夫决策过程的定义，SSP-MMC的贝尔曼方程可设置如下：
\begin{equation}
J(\bm x_\mathrm{k}) = \min\limits_{u_\mathrm{k} \in U_\mathrm{k}(\bm x_\mathrm{k})} E[g(r_\mathrm{k}) + J(\mathrm{LSTM}(\bm x_\mathrm{k},u_\mathrm{k},r_\mathrm{k}))]
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中&  $\bm x_0$ &初始记忆状态；\\
    & $J$ &记忆状态对应的复习总成本；\\
    & $U_\mathrm{k}$ &记忆状态对应的可选复习行为集合\footnote{为了缩小搜索空间，我们将其设置为$[1,h_\mathrm{k}]$}；\\
    & $g$ &记忆状态和复习行为对应的复习代价函数\footnote{为了简化问题，我们只考虑$r_\mathrm{k}$}；
\end{tabularx}\vspace{3.15bp}

基于该方程，可以使用动态规划迭代求解，得到每个$\bm x_\mathrm{k}$对应的最优复习行为$u_\mathrm{k}$。

考虑到$\bm x_\mathrm{t}$的向量值都是连续的，不利于使用矩阵记录状态空间，可以将其离散化：
\begin{equation}
\bm X \xrightarrow[]{(\lfloor \frac{\bm x}{\varepsilon} \rfloor|\bm x \in \bm X)} \bm{\mathrm{X}}
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中& $\bm X$ &记忆状态空间；\\
    & $\bm{\mathrm{X}}$ &离散化的记忆状态空间；\\
    &  $\varepsilon$ &离散化精度；
\end{tabularx}\vspace{3.15bp}

根据LSTM采用$\mathrm{tanh}$作为激活函数，其输出值域为$(-1,1)$，因此离散化后的每一维的离散值是有限的，总计$\lceil\frac{2}{\varepsilon}\rceil$个。

我们可以建立一个成本矩阵$J^{\lceil\frac{2}{\varepsilon}\rceil}$，并将每一项元素初始化为$+\infty$。设置$J[\bm{\mathrm{x_0}}] = 0$，然后开始遍历每个$\bm{\mathrm{x}}$，从$U(\bm{\mathrm{x}})$中遍历每个$u_\mathrm{k}$，计算$p_\mathrm{k}$、$\bm{\mathrm{x}}_\mathrm{r=1}$、$\bm{\mathrm{x}}_\mathrm{r=0}$。
\begin{equation}
\label{eqn:ssp-mmc}
J[\bm{\mathrm{x}}] = \min\limits_{p_\mathrm{k}} [p_\mathrm{k} \cdot (g(1) + J[\bm{\mathrm{x}}_\mathrm{r=1}]) + (1-p_\mathrm{k}) \cdot (g(0) + J[\bm{\mathrm{x}}_\mathrm{r=0}])]
\end{equation}
\begin{tabularx}{\textwidth}{@{}l@{\quad}r@{———}X@{}}
    式中&  $p_\mathrm{k}$ &回忆成功的概率；\\
    &  $\bm{\mathrm{x}}_\mathrm{r=1}$ &回忆成功后的记忆状态；\\
    &  $\bm{\mathrm{x}}_\mathrm{r=0}$ &回忆失败后的记忆状态；
\end{tabularx}\vspace{3.15bp}

通过公式\ref{eqn:ssp-mmc}，算法能迭代更新每个记忆状态对应的最优成本。再使用一个策略矩阵记录每个状态下最优的复习间隔，最终最优策略会在一遍遍迭代中收敛。

\section{间隔重复模拟环境}[Stimulation Environment]

为了验证我们的复习调度算法的有效性，我们基于LSTM-HLR模型构建了一个间隔重复模拟环境，用于比较不同复习调度算法的性能。在间隔重复模拟环境中，我们模拟学习者从一个单词集合中挑选回忆的对象，并根据LSTM-HLT模型来生成回忆结果和记忆状态。复习调度算法根据回忆结果和记忆状态来安排下一个复习时机。

模拟过程涉及两个维度，一个是日间，一个是日内。为了模拟学习者的备考期限和每日学习时间限制，环境限制了模拟进行的天数和每天用于复习和学习的时间。然而，由于回忆结果的随机性，复习调度也受回忆结果的影响，因此可能出现当日复习安排耗时超过每日时间限制的情况。为了尽量避免这种情况，我们在模拟过程中优先保证复习任务完成。当每日时间耗尽时，不论复习任务是否完成，都延迟到第二天进行。

模拟流程如算法\ref{alg:stimulation}所示。

\begin{algorithm}[htbp]
    \KwData{$Student,Schedule,day_\mathrm{limit},cost_\mathrm{limit}$}
    \KwResult{$Result$}
    \SetKw{Break}{break}
    \For{$day \leftarrow 1$ \KwTo $day_\mathrm{limit}$}{
        $cost \leftarrow 0$\;
        \ForEach{$w \in rewiew[day]$}{
            \uIf{$cost >= cost_\mathrm{limit}$}{
                \Break
            }
            $w.t = \leftarrow day - w.last$\;
            $w.p \leftarrow 2^{-\frac{w.ivl}{w.h}}$\;
            \eIf{$random() < w.p$}{
                $w.r=1$\;
                $cost \leftarrow cost + cost_\mathrm{r=1}$\;
            }{
                $w.r=0$\;
                $cost \leftarrow cost + cost_\mathrm{r=0}$\;
            }
            $w.x \leftarrow Student.review(w.x,w.r,w.t,w.p)$\;
            $rewiew[day+Schedule(w.x)].add(w)$\;
            $w.last \leftarrow day$\;
        }
        \ForEach{$w \in new$}{
            \uIf{$cost >= cost_\mathrm{limit}$}{
                \Break
            }
            $cost \leftarrow cost + cost_\mathrm{new}$\;
            $w.x \leftarrow Student.new()$\;
            $rewiew[day+Schedule(w.x)].add(w)$\;
            $w.last \leftarrow day$\;
            $new.pop(w)$\;
        }
    }
\caption{间隔重复模拟环境}
\label{alg:stimulation}
\end{algorithm}

\section{间隔重复系统架构}[Architecture]

为了让我们的算法能够被应用于实际环境，我们设计了一套间隔重复系统架构。

将数据集构建模块、记忆预测模块、复习调度模块相结合，可以得到端到端的间隔重复系统。由于这些模块对存储和算力的要求很高，所以我们将整个间隔重复系统拆分为远程和本地两个部分。

\begin{figure}[htbp]
\centering
\includegraphics[width = 0.9\textwidth]{framework}
\caption{间隔重复系统架构}
\label{fig:framework}
\end{figure}

图\ref{fig:framework}展示了间隔重复系统的框架，它分为两个主要部分：绿色为本地，红色为远程。

每次学习者复习单词时，带有时间序列信息的记忆行为事件将被记录在本地，我们称之为记忆行为日志。在学习者完成当天的所有复习后，这些日志会被上传到远程。

完整的记忆行为日志必须处理大量的写入，需要永久保留，而且一旦写入就不会更新，所以我们使用流式数据服务将日志写入数据仓库。在预处理ETL中，我们定期计算所有单词的难度，并汇总LSTM-HLR记忆模型所需的训练数据，为SSP-MMC调度算法提供训练环境。

在计算出模型参数和最优策略后，远程将相关配置推送到本地。然后本地预测器和调度器将加载新的配置来预测记忆状态，并为学习者学习的每个单词安排复习日期。

通过将训练任务全部部署至远程，该框架对学习者的设备要求较低。同时，配置推送可以按一定的周期进行，不需要实时同步，对服务器的并发处理能力要求不高。

\section{本章小结}[Summary]

本章依次介绍了记忆预测模型、复习规划算法、间隔重复模拟环境和间隔重复系统架构。

在记忆预测模型中，为了解决目前开源数据集缺少时序特征对问题，我们构建了新的记忆行为数据架构，尽可能包含完整的记忆行为序列信息。然后将常见的循环神经网络LSTM引入至HLR模型来估计记忆半衰期。

在复习规划算法中，我们提出了最小化记忆成本的优化目标，并将间隔重复转化为一个随机最短路径问题。通过使用LSTM-HLR预测记忆系统动态，结合分布式异步值迭代方法，提出了SSP-MMC算法来计算每个记忆状态下最佳的复习行为。

在间隔重复模拟环境中，我们将LSTM-HLR作为学生模型，并构建了每日学习成本限制和学习期限限制，以在相同的环境下对比不同复习算法的性能表现。

在间隔重复系统架构中，为了使复习调度算法能够应用于现实环境中，我们拆分了算法训练和算法调用，将两者分布部署至远程和本地，以降低对客户端和服务端的性能要求。

\chapter{间隔重复系统实验}[Experiment]

\section{构建数据集}[Dataset]

本实验收集了1个月的墨墨背单词学习者学习日志，其包含2.2亿条记忆行为数据。数据集构建过程如下：

（1）一位学习者对一个单词的一次复习将产生一条记忆行为数据。其字段包括单词id、学习者id、时间戳、反馈。

（2）当学习者完成当日的学习任务后，当日所有的记忆行为数据将以日志的形式上传至服务器。

（3）在服务器上，日志同步系统将学习者学习日志结构化，写入数据库。

（4）按照学习者和单词进行分组，计算每次复习之间的间隔。并将每次的反馈和间隔按先后顺序拼接，得到反馈序列和间隔序列。

（5）按照单词难度、反馈序列、间隔序列分组，计算不同间隔下的回忆概率。并将每次复习的回忆概率按先后顺序拼接，得到回忆概率序列。为了获得每个难度的单词在不同记忆行为历史下的半衰期和回忆概率，本实验对数据进行了聚合，最终得到7万条复习记录

\section{对比模型与评估指标}[Model and metric]

\subsection{记忆预测模块}[Memory predict module]

本实验采用了两个指标进行综合比较：

\begin{itemize}
    \item MAE(p)：计算预测的回忆概率与实际统计的回忆概率之间的绝对误差。回忆概率是一个落在区间中的连续值，MAE越小，预测越准确；
    \item MAPE(h)：计算预测的记忆半衰期与实际半衰期之间的绝对百分比误差。之所以对记忆半衰期不使用MAE而使用MAPE评估，是因为考虑到半衰期的现实意义。偏差10\%和偏差10天，对于1天的半衰期和100天的半衰期而言，前者更符合实际。
\end{itemize}

本实验对比三类间隔重复模型及其变体：

\begin{itemize}
    \item LSTM-HLR，我们在\ref{sec:LSTM}节所描述的模型。为了进行消融实验，考虑了4种变体：有和没有$\bm{\Delta t}_\mathrm{1:i-1}$特征（+t），以及有和没有$\bm p_\mathrm{1:i-1}$特征（+p）；
    \item HLR，遗忘预测模型的对比基线，考虑两种变体：有和没有单词特征（+l）；
    \item SM-2，传统的启发式模型，与算法\ref{alg:sm-2}中描述一致。
\end{itemize}

\subsection{复习调度模块}[Review schedule module]

我们将SSP-MMC与几个基线调度算法进行对比：

\begin{itemize}
    \item Random策略，每次从$[1,h_N]$中随机选择一个间隔进行安排复习。
    \item Anki，SM-2的一种变体，参考其开源的代码。由于Anki的算法中，学习者的回忆结果是以1-4分输入。我们将回忆失败映射到1分，回忆成功映射到3分。
    \item 半衰期，即直接以半衰期作为本次安排的复习间隔。
    \item 固定阈值，即当$p$小于等于某一水平（我们使用80\%）时进行复习。
    \item Memorize\cite{tabibianEnhancingHumanLearning2019}，一种基于最优控制的算法，代码来自于他们开源的仓库。
\end{itemize}

我们的评价指标包括：

\begin{itemize}
    \item 达到目标半衰期的记忆数量（target half-life reached, THR）
    \item 累计记忆期望，即学习者所有记忆材料的回忆概率之和（summation of recall probability, SRP）
    \item 累计新增的记忆数量（words total learned, WTL）
\end{itemize}

\section{实验流程}[Procedure]

\subsection{实验步骤}[Step]

第一步，我们使用构建好的数据集来训练记忆预测模型，得到LSTM-HLR。

第二步，为了确定合适的隐藏层节点数量，实验对比了不同节点数的误差情况，以确定SSP-MMC的状态空间维数。

第三步，训练SSP-MMC得到最优复习调度策略，其中LSTM-HLR作为状态转移函数调用。

第四步，我们使用LSTM-HLR搭建了一个复习模拟环境，模拟SSP-MMC和几个对比算法的复习情况，以评估算法效果。

\subsection{训练LSTM-HLR}[Train LSTM-HLR]

本实验随机抽取20\%的数据用于训练，最终确定了以下参数：迭代次数=500000，学习率=0.0005，权重衰减系数=0.0001，隐藏层节点数=16。HLR模型的参数使用相同的数据进行训练，确定了以下参数：迭代次数=7500000，学习率=0.001，$\alpha$=0.002，$\lambda$=0.01。SM-2模型不需要训练。对于剩余的80\%数据，使用5次重复的2折交叉验证\cite{dietterichApproximateStatisticalTests1998}进行评估。

\subsection{对比损失函数}[Compare Loss Function]

不同的损失函数，对记忆半衰期和回忆概率的预测结果可能会有较大的影响，本实验对比了MAPE和L1两类损失函数，以验证我们对损失函数的假设。

\subsection{选择状态空间维度}[Select dimension of state space]

随着隐藏层节点数量提高，SSP-MMC的状态空间大小呈指数级增长。由于SSP-MMC的训练时间复杂度为$\mathrm O(k\lceil\frac{2}{\varepsilon}\rceil^n)$，如果状态空间的维数过高，SSP-MMC将无法在合理的时间内计算出结果。因此，研究状态空间维数与预测误差之间的关系，有助于挑选合适的隐藏层节点数量，以在预测精度与训练难度之间取得平衡。

\subsection{训练SSP-MMC}[Train SSP-MMC]

算法\ref{alg:ssp_mmc}描述了SSP-MMC的训练流程。

\begin{algorithm}[htbp]
    \KwData{$a,b,\bm{\mathrm{x}}_{N}$}
    \KwResult{$\pi,J$}
    $J = \inf$\;
    $J[\bm{\mathrm{x}}_{N}] = 0$\;
    \While{$\Delta J<0.1$}{
        $J_0=J[\bm{\mathrm{x}}_0]$\;
        \For{$\bm{\mathrm{x}}\leftarrow \bm{\mathrm{x}}_0$ \KwTo $\bm{\mathrm{x}}_{N-1}$}{
            \ForEach{$u \in U(\bm{\mathrm{x}})$}{
                $p\leftarrow2^{-\frac{u}{h}}$\;
                $\bm{\mathrm{x}}_\mathrm{r=1}\leftarrow \mathrm{LSTM}(\bm x,u,1)$\;
                $\bm{\mathrm{x}}_\mathrm{r=0}\leftarrow \mathrm{LSTM}(\bm x,u,0)$\;
                $J\leftarrow p\cdot (a + J[\bm{\mathrm{x}}_\mathrm{r=1}]) + (1-p)\cdot (b + J[\bm{\mathrm{x}}_\mathrm{r=0}])$\;
                \If{$J< J[\bm{\mathrm{x}}]$}{
                    $J[\bm{\mathrm{x}}]=J$\;
                    $\pi[\bm{\mathrm{x}}]=u$\;
                }
            }
        }
        $\Delta J= J_0 - J[\bm{\mathrm{x}}_0]$\;
    }
\caption{SSP-MMC}
\label{alg:ssp_mmc}
\end{algorithm}

$a$是回忆成功的复习成本，$b$是回忆失败的复习成本，$\bm{\mathrm{x}}_{N}$是记忆的目标状态。

\subsection{模拟复习}[Stimulate review]

我们设定360天（接近一年）的回忆半衰期为目标半衰期，当材料的记忆半衰期超过这个值时，它将不会被安排复习。然后，考虑到实际场景中学习者每天的学习时间大致恒定，我们设定600秒（10分钟）为每天学习成本的上限。当每次学习和复习过程中的累计成本超过这个上限时，无论是否完成，复习任务都会被推迟到第二天，以确保每种算法在相同的记忆成本下进行比较。我们使用学习者的平均时间，即成功回忆的时间为3秒，失败回忆的时间为9秒。最后，语言学习是一个长期的过程，我们设定模拟时间为360天。

\chapter{实验分析与总结}[Result]

\section{长期记忆预测结果}[Stimulate review]

表\ref{tab:result:predict}展示了SM-2、HLR、LSTM-HLR以及对应消融实验的结果。图\ref{fig:result:predict}展示了各模型的预测分布情况。

\begin{table}[htbp]
    \caption{预测结果评估}
    \label{tab:result:predict}
    \vspace{0.5em}\centering\wuhao
    \begin{tabular}{lll}
    \toprule[1.5pt]
    \textbf{Model}  &\textbf{MAE(p)}$\downarrow$ &\textbf{MAPE(h)}$\downarrow$\\
    \midrule[1pt]
    LSTM-HLR +p +t  &0.0147   &7.01\%    \\
    LSTM-HLR +p     &0.0197   &9.66\%    \\
    LSTM-HLR +t     &0.0249   &13.13\%    \\
    LSTM-HLR        &0.0345   &18.62\%    \\
    \midrule[1pt]
    HLR +lex        &0.0828   &130.85\%    \\
    HLR             &0.0965   &159.41\%   \\
    \midrule[1pt]
    SM-2            &0.1304   &163.15\%    \\
    \midrule[1pt]
    $\bar{p}=0.77$  &0.0754   &N/A        \\
    \bottomrule[1.5pt]
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:result:predict:lstm}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[LSTM]{\includegraphics[width=0.45\textwidth]{lstm-mape-distribution}}}
    \hspace{2em}
    \subfigure{\label{fig:result:predict:lstm_i}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[LSTM +t]{\includegraphics[width=0.45\textwidth]{lstm_i-mape-distribution}}}
    \end{minipage}
    \centering
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:result:predict:lstm_p}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[LSTM +p]{\includegraphics[width=0.45\textwidth]{lstm_p-mape-distribution}}}
    \hspace{2em}
    \subfigure{\label{fig:result:predict:lstm_p_i}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[LSTM +p+t]{\includegraphics[width=0.45\textwidth]{lstm_p_i-mape-distribution}}}
    \end{minipage}
    \centering
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:result:predict:hlr_l}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[HLR +l]{\includegraphics[width=0.45\textwidth]{hlr_l-mape-distribution}}}
    \hspace{2em}
    \subfigure{\label{fig:result:predict:sm2}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[SM-2]{\includegraphics[width=0.45\textwidth]{sm2-mape-distribution}}}
    \end{minipage}
    \vspace{0.2em}
    \caption{预测误差与样本分布}
    \label{fig:result:predict}
\end{figure}

通过观察表\ref{tab:result:predict}我们可以看到，同时带有$p$和$t$特征的LSTM-HLR +p +t模型表现最好；其次是只带$p$特征的LSTM-HLR +p。并且，使用时序特征的所有模型在所有指标上都优于只使用统计特征的HLR。我们认为这样的结果是基于一个事实：一个学习者连续记住同一内容三次再连续遗忘同一内容三次，与先连续遗忘三次再连续记住三次，这两个过程是截然相反的。HLR仅考虑了历史累计正确次数和遗忘次数，无法区分这两个过程，只能在训练过程中给出折衷的预测，误差较大。

观察图\ref{fig:result:predict:lstm}至\ref{fig:result:predict:sm2}的柱状图，可以看到记忆半衰期分布集中在1天至10天。通过对比各模型的MAPE(h)分布，可以发现LSTM-HLR系列模型在短半衰期区间的MAPE(h)明显小于HLR系列模型。我们认为这是HLR的损失函数式\ref{eqn:hlr:loss}中的项所致。该项无差别地惩罚任意半衰期区间的误差，从而使低半衰期区间的百分比误差很大。本文的LSTM-HLR模型通过改进损失函数克服了这个问题。虽然在长半衰期区间上，HLR系列模型的误差低于LSTM-HLR系列模型。但长半衰期区间的样例较少，对最终平均误差的影响较小。从现实角度来看，当记忆半衰期变长，学习者在下一次复习之前在背单词APP外进行学习的可能性越来越大，我们能收集到的记忆行为数据也会偏离学习者的真实情况。对于这些有偏差的数据，做出更准确预测的实际意义较小。

关于特征为何能降低预测的误差，我们认为这与心理学中记忆的提取强度与存储强度有关。较难提取的记忆经回忆会得到更多的强化\cite{bjorkNewTheoryDisuse1992}。在我们的模型中，历史的回忆概率可以反映出每次回忆的提取强度，而半衰期则类似于存储强度。因此，回忆概率历史对预测半衰期是有用的。

每次复习之间的间隔也是一个重要的信息。以（0-2-4-6-8）的间隔复习，与以（0-5-5-5-5）的间隔复习，效果会有显著的不同\cite{maddoxRoleForgettingRate2011}，这一点在我们的实验中也得到了印证。

\section{对比损失函数结果}[Compare Loss Function]

表\ref{tab:loss}展示了MAPE和L1二者作为LSTM-HLR损失函数后预测误差的结果。图\ref{fig:loss}展示了预测误差的分布情况。

\begin{table}[htbp]
    \caption{预测结果评估}
    \label{tab:loss}
    \vspace{0.5em}\centering\wuhao
    \begin{tabular}{llll}
    \toprule[1.5pt]
    \textbf{Model}  &\textbf{MAE(p)}$\downarrow$ &\textbf{MAPE(h)}$\downarrow$ &\textbf{MAE(h)}$\downarrow$\\
    \midrule[1pt]
    LSTM-HLR +MAPE  &0.0147   &7.01\%   &2.2218\\
    LSTM-HLR +L1    &0.0165   &9.66\%   &2.1251\\
    \bottomrule[1.5pt]
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:loss:mape}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[MAPE]{\includegraphics[width=0.45\textwidth]{lstm_p_i_16_mape-mape-distribution}}}
    \hspace{2em}
    \subfigure{\label{fig:loss:l1}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[L1]{\includegraphics[width=0.45\textwidth]{lstm_p_i_16_f1-mape-distribution}}}
    \hspace{2em}    \end{minipage}    
    \vspace{0.2em}
    \caption{不同损失函数的误差对比}
    \label{fig:loss}
\end{figure}

从表\ref{tab:loss}中我们可以看出，虽然LSTM-HLR +L1在MAE(h)的误差上要低于LSTM-HLR +MAPE，但其在MAE(p)上的误差依然高于LSTM-HLR +MAPE。这可以由图\ref{fig:loss:mape}和图\ref{fig:loss:l1}之间的差异来解释。LSTM-HLR +L1的误差主要集中在记忆半衰期$[0,10]$的区间内，而这正是因为L1损失函数在惩罚误差时，不考虑记忆半衰期的相对大小。LSTM-HLR +MAPE在这一区间内的误差则都小于10\%，虽然其在记忆半衰期为$10^3$左右时的误差较大，但这一区间的样本较少，对总体而言影响不大。至于

\section{选择状态空间维数结果}[Select Dimension]

我们对比了2、4、8、16、32个隐藏层节点的效果，如图\ref{fig:dimension}所示。

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:dimension:train}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[训练集误差]{\includegraphics[width=0.45\textwidth]{select_dim_train}}}
    \hspace{2em}
    \subfigure{\label{fig:dimension:test}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[测试集误差]{\includegraphics[width=0.45\textwidth]{select_dim_eval}}}
    \end{minipage}
    \centering
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:dimension:smooth}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[平滑对比]{\includegraphics[width=0.45\textwidth]{select_dim_smooth}}}
    \hspace{2em}
    \subfigure{\label{fig:dimension:trand}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[趋势对比]{\includegraphics[width=0.45\textwidth]{select_dim_trend}}}
    \end{minipage}
    \vspace{0.2em}
    \caption{不同隐藏层节点数的误差对比}
    \label{fig:dimension}
\end{figure}

增加隐藏层节点数会指数级提高状态空间大小，提高复习调度算法的复杂度。而减少隐藏层节点数会提高长期记忆预测的误差。经过权衡，我们将隐藏层节点数固定为4。

\section{复习规划模拟结果}[Stimulate review]

\begin{figure}[htbp]
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:thr}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[THR]{\includegraphics[width=0.45\textwidth]{THR}}}
    \hspace{2em}
    \subfigure{\label{fig:srp}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[SRP]{\includegraphics[width=0.45\textwidth]{SRP}}}
    \end{minipage}
    \centering
    \begin{minipage}{\textwidth}
    \centering
    \subfigure{\label{fig:wtl}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[WTL]{\includegraphics[width=0.45\textwidth]{WTL}}}
    \hspace{2em}
    \subfigure{\label{fig:new}}\addtocounter{subfigure}{-2}
    \subfigure{\subfigure[NEW]{\includegraphics[width=0.45\textwidth]{new}}}
    \end{minipage}
    \vspace{0.2em}
    \caption{模拟学习结果}
    \label{fig:simulation}
\end{figure}

图\ref{fig:simulation}展示了模拟环境下各复习规划算法的表现。

在指标THR上（图\ref{fig:thr}），SSP-MMC的表现比所有基线都好，这并不令人惊讶。THR与SSP-MMC的优化目标一致，SSP-MMC可以达到这个指标的上限。

为了量化每种算法性能之间的相对差异，我们比较了THR=1000的天数（图中标记为$\star$）：SSP-MMC为202天，THRESHOLD为238天，而其余算法在模拟结束前都没有达到这一标准。与THRESHOLD相比，SSP-MMC节省了15.12\%的复习时间。

在指标SRP上(图\ref{fig:srp})的结果与THR上的结果相似。这意味着学习者按照SSP-MMC的时间表学习会记得最多。

在指标WTL上（图\ref{fig:wtl}），RANDOM击败了所有的算法，因为只要调度算法不安排复习，学习者就可以继续学习新的单词，但这是以忘记已经学过的单词为代价的。此外，SSP-MMC胜过其他基线，因为它将记忆的成本降到最低，给学习者更多的时间来学习新单词。

\section{研究结果总结}[Summary of Findings]

本项工作的目的有三个：第一，在前人的基础上，改进记忆预测模型，降低长期记忆预测误差。第二，提出间隔重复调度算法的优化目标，并构建算法计算最优策略。第三，将记忆预测模型和间隔重复调度算法整合至一个系统。

我们研究了使用不同时序特征时预测误差的变化，并对损失函数进行了调整。我们发现只要引入时序特征，就能显著降低长期记忆的预测误差。同时，使用绝对百分比误差的效果要优于绝对误差和均方误差。此外，通过提高LSTM隐藏层节点数量可以降低预测误差，但效果有限。

在间隔重复调度问题上，这项工作的主要目标是在相同复习期限和每日学习成本约束下对比不同算法的长期记忆达成量和记住的材料期望数量。我们进行了模拟实验，使用最小化复习成本来训练我们的复习调度算法，然后在达到目标半衰期的记忆数量THR、累计记忆期望SRP和累计新增的记忆数量WTL三个指标上评估算法表现。我们观察到，在THR和SRP两个指标上，SSP-MMC得到的最优策略超越了所有对比基线，这也是我们预期的，而WTL指标存在漏洞。

而由于项目时间限制，我们没有将间隔重复系统完整地部署上线。

\section{研究限制}[Limitations]

本项工作的限制主要有以下三点：

\begin{itemize}
    \item 我们的数据仅来自于墨墨背单词，可能在语言学习以外的科目上表现不佳。
    \item 在聚合数据的过程中，我们忽略了学生记忆能力的差异，因为我们预计这将会进一步加重数据稀疏的问题。这种忽略可能会导致最终的复习调度算法只对平均水平的学生而言是最优的。
    \item 在选择状态空间维数时，由于SSP-MMC的算法复杂度随着维数指数增加，为了在合理的时间内计算出最优策略，我们以牺牲预测精度为代价，仅在LSTM-HLR中应用了4个隐藏层节点。这种牺牲可能导致最优策略于实际的情况有一定的偏差。
\end{itemize}

\section{可持续性和伦理道德}[SustainabilityandEthics]

这项工作基本上不会对社会构成任何重大的可持续性或伦理道德问题。

从可持续的角度来看，唯一的问题是训练LSTM-HLR和SSP-MMC的计算时间会很长，从而消耗更多的能量。

从伦理的角度来看，一个问题是用于训练LSTM-HLR和SSP-MMC的数据应足够通用，以免受任何形式的偏见影响。与任何数据驱动的应用程序一样，始终牢记数据隐私问题。当这些系统部署在生产环境中时，学生学习过程的私密性就变得非常重要，这也有助于防止学生因记忆知识需要更长的时间而被嘲笑的欺凌事件。学生在复习中的表现也会影响他们的积极性和自尊心，因此在设计为学生服务的间隔重复系统时必须考虑这些因素。在这项工作中，我们对数据进行了脱敏处理，尽可能地避免学生隐私的泄露。

另一个主要的伦理问题可能是人工智能接管了人类的教学。但是，这种情况发生的可能性需要很长时间。这对下一代人记忆知识可能有很大的帮助，但要达到这一境界，还需要付出很大的努力。例如，人类教师在进行复习测试时也可以考虑其他因素，例如面部表情、语调等，以确定学生是否确定自己的答案。实现类似的目标意味着使用单独的人工智能系统来表达面部表情、语音等。阅读或使用一个非常复杂的系统，将所有这些任务一起完成以做出决策。人类教师还采取措施确保学生不会在考试中作弊。另一方面，人类教师有时对某些学生表现出偏见，而这是目前的复习调度算法无法实现的。

\section{未来工作}[Future Work]

LSTM-HLR模型在预测记忆方面有了较大的提升，但尚未考虑学习者自身因素对长期记忆的影响，考虑学习者特征对此记忆状态转移的影响是一个很有吸引力的方向。此外，在语言学习以外的场景进行相似的工作也是值得的，可以验证本工作所提出的方法的普适性。最后，学习者使用间隔重复方法的场景十分多样化，根据学习者的目标来调整SSP-MMC算法的优化目标也是一个值得研究的问题。

\section{本章小结}[Summary]

本章中，我们在记忆预测和复习规划两个方面对比已有的算法，其中还涉及了结合LSTM-HLR和SSP-MMC时考虑的状态空间维数问题。

在长期记忆预测上，LSTM-HLR算法超过了目前最先进的算法。并在消融实验中，我们发现在仅使用反馈序列特征也优于HLR模型。间隔序列特征和回忆概率序列特征都对模型的预测有正面贡献，其中回忆概率序列特征的贡献更大。并且间隔序列特征和回忆概率序列特征的贡献分别起作用的，当结合两个特征使用时，结果优于单独仅用其中一个特征。

在状态空间维数选择的过程中，我们发现随着隐藏层节点数量的增加，预测误差的下降较为缓慢，而状态空间的复杂度呈指数增加。

在复习规划上，SSP-MMC在由LSTM-HLR构建的间隔重复模拟环境中超过了所有对比的算法。

本项工作的研究结果确定了时序特征在长期记忆预测中的重要作用，同时证明了绝对百分比误差在预测记忆半衰期时是一个较好的损失函数。在复习规划问题中，研究结果反映了以最小化记忆成本为优化目标，能够达到最大化记忆期望的结果。

本项工作的限制主要集中在数据范围有限上，没有区分不同学习者的差异，以及在其他学科上的有效性。此外，SSP-MMC算法的复杂度也限制其性能进一步提升。

在可持续性和伦理道德上，本项工作基本上没有问题。不如说，通过自动化复习安排，节约人类教师的精力用于更重要的地方正是本项工作的价值之一。

在未来工作上，探究学习者内部差异和探索更多的学习科目、更个性化的复习目标都是具有诱惑力的工作方向。